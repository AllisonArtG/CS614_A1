{"cells":[{"cell_type":"markdown","metadata":{"id":"2FpU7fMEdFho"},"source":["**Setup and Dataset Preprocessing**"]},{"cell_type":"markdown","metadata":{"id":"9hWpVCAlBxll"},"source":["Imports"]},{"cell_type":"code","execution_count":49,"metadata":{"executionInfo":{"elapsed":162,"status":"ok","timestamp":1682739468468,"user":{"displayName":"Allison Gong","userId":"03630911304537870557"},"user_tz":240},"id":"28Eh6VzZdBIJ"},"outputs":[],"source":["from google.colab import drive, files\n","import copy, os, shutil, time\n","from zipfile import ZipFile\n","import torch\n","from torch import nn, cuda, device, optim\n","from torch.utils.data import Dataset, random_split, DataLoader\n","from torchvision import datasets, models, transforms\n","from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import cv2, glob"]},{"cell_type":"markdown","metadata":{"id":"FKhJwUW2l0zv"},"source":["Dataset directory name and it's intended path"]},{"cell_type":"code","execution_count":50,"metadata":{"executionInfo":{"elapsed":209,"status":"ok","timestamp":1682739468881,"user":{"displayName":"Allison Gong","userId":"03630911304537870557"},"user_tz":240},"id":"bGP4yvX3qhIc"},"outputs":[],"source":["dir_name = \"indian_birds_dataset\"\n","dataset_path = os.path.join(\"/content\", dir_name)"]},{"cell_type":"markdown","metadata":{"id":"DkDHergJLOJb"},"source":["Mount Google Drive locally, to access dataset - `archive.zip`."]},{"cell_type":"code","execution_count":51,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":977,"status":"ok","timestamp":1682739469852,"user":{"displayName":"Allison Gong","userId":"03630911304537870557"},"user_tz":240},"id":"GIiImZH8LZxy","outputId":"069816ec-e335-4925-d16f-7e99719b258f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["drive.mount(\"/content/drive\")"]},{"cell_type":"markdown","metadata":{"id":"kv4Ou5MCRPUd"},"source":["Extracts the dataset into `\"/content\"`. **NOTE**: Replace the value `dataset_path` with the path to `archive.zip` in Google Drive (e.g. `\"/content/drive/MyDrive/.../archive.zip\"`)."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"xHkfp8vKNCDi"},"outputs":[],"source":["dataset_zip_path = \"/content/drive/MyDrive/CS614/archive.zip\"\n","\n","with ZipFile(dataset_zip_path, \"r\") as zip:\n","  zip.extractall(\"/content\")"]},{"cell_type":"markdown","metadata":{"id":"cxFCdiO1Ddsf"},"source":["Moves the dataset's root directory from `\"training_dataset\"` to `\"indian_birds_dataset\"`"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"tVJuZoSdyh2p"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/indian_birds_dataset/training_set'"]},"execution_count":53,"metadata":{},"output_type":"execute_result"}],"source":["shutil.move(\"/content/training_set\", dataset_path)"]},{"cell_type":"markdown","metadata":{"id":"s8iE9rZAR7cp"},"source":["Delete accidental directory\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Dm_W2VSMRYn8"},"outputs":[],"source":["def delete_dir(path):\n","  if os.path.isdir(path):\n","    print(f\"Deleting {path}\")\n","    shutil.rmtree(path, ignore_errors=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"7fYgVEiVSAZO"},"outputs":[{"name":"stdout","output_type":"stream","text":["Deleting /content/indian_birds_dataset/training_set\n"]}],"source":["delete_dir(os.path.join(dataset_path, \"training_set\"))"]},{"cell_type":"markdown","metadata":{"id":"QArdi05TtSpW"},"source":["Static dataset and model variables"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"UlpxwixMtNJL"},"outputs":[],"source":["input_size = 299\n","num_classes = 25\n","batch_size_train = 20\n","batch_size_vt = 20\n","num_epochs = 1 \n","num_workers = 2\n","learning_rate = 0.0001\n","momentum = 0.9\n","\n","dataset_split = {\"train\" : 0.8, \"valid\" : 0.1, \"test\": 0.1}"]},{"cell_type":"markdown","metadata":{"id":"4XhB09J2wNdY"},"source":["Model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"eJffIkf4wMLL"},"outputs":[],"source":["model = models.resnet34(weights=\"DEFAULT\")\n","num_features = model.fc.in_features\n","model.fc = nn.Linear(num_features, num_classes)"]},{"cell_type":"markdown","metadata":{"id":"irC8vtseV45H"},"source":["Prepare the Dataset"]},{"cell_type":"markdown","metadata":{"id":"OtZCGFvn6occ"},"source":["Because `random_split` doesn't actually split the dataset, but just keeps the indices of the subsets. This class allows me to apply different transformations to the training dataset vs. the testing and validation datasets."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"4fbPimDjowA0"},"outputs":[],"source":["class WrapperDataset(Dataset):\n","    def __init__(self, subset, transform=None):\n","        self.subset = subset\n","        self.transform = transform\n","        \n","    def __getitem__(self, index):\n","        x, y = self.subset[index]\n","        if self.transform:\n","            x = self.transform(x)\n","        return x, y\n","        \n","    def __len__(self):\n","        return len(self.subset)"]},{"cell_type":"markdown","metadata":{"id":"3tjl-hx6H_k8"},"source":["Dataset Preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"6CVZNIE7W-mF"},"outputs":[],"source":["full_dataset = datasets.ImageFolder(dataset_path)\n","\n","class_to_idx = full_dataset.class_to_idx\n","\n","generator = torch.Generator().manual_seed(42)\n","_train, _valid, _test = random_split(full_dataset, [x for x in dataset_split.values()], generator=generator)\n","\n","num_total_samples = len(_train.dataset)\n","num_samples = {}\n","for i in range(len(dataset_split)):\n","  ds = list(dataset_split.keys())[i]\n","  num_samples[ds] = int(dataset_split[ds] * num_total_samples)\n","\n","train_transform = transforms.Compose([\n","    transforms.Resize(input_size),\n","    transforms.CenterCrop(input_size),\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","])\n","\n","valid_test_transform = transforms.Compose([\n","    transforms.Resize(input_size),\n","    transforms.CenterCrop(input_size),\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","])\n","\n","train = WrapperDataset(_train, transform=train_transform)\n","test = WrapperDataset(_test, transform=valid_test_transform)\n","valid = WrapperDataset(_valid, transform=valid_test_transform)\n","\n","train_loader = DataLoader(train , batch_size=batch_size_train, num_workers=num_workers,  shuffle=True)\n","valid_loader = DataLoader(valid, batch_size=batch_size_vt, num_workers=num_workers)\n","test_loader = DataLoader(test, batch_size=batch_size_vt, num_workers=num_workers)"]},{"cell_type":"markdown","metadata":{"id":"oXTQ-sUbp1Pt"},"source":["List of classes and mapping from index to class"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"stbEtLWYV4Gk"},"outputs":[{"name":"stdout","output_type":"stream","text":["{0: 'Asian Green Bee-Eater', 1: 'Brown-Headed Barbet', 2: 'Cattle Egret', 3: 'Common Kingfisher', 4: 'Common Myna', 5: 'Common Rosefinch', 6: 'Common Tailorbird', 7: 'Coppersmith Barbet', 8: 'Forest Wagtail', 9: 'Gray Wagtail', 10: 'Hoopoe', 11: 'House Crow', 12: 'Indian Grey Hornbill', 13: 'Indian Peacock', 14: 'Indian Pitta', 15: 'Indian Roller', 16: 'Jungle Babbler', 17: 'Northern Lapwing', 18: 'Red-Wattled Lapwing', 19: 'Ruddy Shelduck', 20: 'Rufous Treepie', 21: 'Sarus Crane', 22: 'White Wagtail', 23: 'White-Breasted Kingfisher', 24: 'White-Breasted Waterhen'}\n"]}],"source":["classes = list(class_to_idx.keys())\n","\n","idx_to_class = {}\n","for clas, index in class_to_idx.items():\n","  idx_to_class[index] = clas\n","print(idx_to_class)"]},{"cell_type":"markdown","metadata":{"id":"NG0CGt2EZ8en"},"source":["GPU/CPU"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Pd34wqAwZ67G"},"outputs":[{"name":"stdout","output_type":"stream","text":["device: cpu\n"]}],"source":["devic = device(\"cuda:0\" if cuda.is_available() else \"cpu\")\n","print(\"device:\", devic)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"4tlV3_CIxJTt"},"outputs":[],"source":["model = model.to(devic)"]},{"cell_type":"markdown","metadata":{"id":"xTGzFO3POUyA"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"lk08bgXZJF0h"},"source":["**Testing**"]},{"cell_type":"markdown","metadata":{"id":"2dS_ra34ntQ8"},"source":["Load trained model's weights\n","\n","**NOTE:** Replace the value of `model_drive_path` with the path to the model weights file, `model`, in Google Drive (e.g. `\"/content/drive/MyDrive/.../model\"`)."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"J3HEb2m_nsE2"},"outputs":[{"name":"stdout","output_type":"stream","text":["odict_keys(['conv1.weight', 'bn1.weight', 'bn1.bias', 'bn1.running_mean', 'bn1.running_var', 'bn1.num_batches_tracked', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.bn1.bias', 'layer1.0.bn1.running_mean', 'layer1.0.bn1.running_var', 'layer1.0.bn1.num_batches_tracked', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.bn2.bias', 'layer1.0.bn2.running_mean', 'layer1.0.bn2.running_var', 'layer1.0.bn2.num_batches_tracked', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.bn1.bias', 'layer1.1.bn1.running_mean', 'layer1.1.bn1.running_var', 'layer1.1.bn1.num_batches_tracked', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer1.1.bn2.bias', 'layer1.1.bn2.running_mean', 'layer1.1.bn2.running_var', 'layer1.1.bn2.num_batches_tracked', 'layer1.2.conv1.weight', 'layer1.2.bn1.weight', 'layer1.2.bn1.bias', 'layer1.2.bn1.running_mean', 'layer1.2.bn1.running_var', 'layer1.2.bn1.num_batches_tracked', 'layer1.2.conv2.weight', 'layer1.2.bn2.weight', 'layer1.2.bn2.bias', 'layer1.2.bn2.running_mean', 'layer1.2.bn2.running_var', 'layer1.2.bn2.num_batches_tracked', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.bn1.bias', 'layer2.0.bn1.running_mean', 'layer2.0.bn1.running_var', 'layer2.0.bn1.num_batches_tracked', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.bn2.bias', 'layer2.0.bn2.running_mean', 'layer2.0.bn2.running_var', 'layer2.0.bn2.num_batches_tracked', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.0.downsample.1.bias', 'layer2.0.downsample.1.running_mean', 'layer2.0.downsample.1.running_var', 'layer2.0.downsample.1.num_batches_tracked', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight', 'layer2.1.bn1.bias', 'layer2.1.bn1.running_mean', 'layer2.1.bn1.running_var', 'layer2.1.bn1.num_batches_tracked', 'layer2.1.conv2.weight', 'layer2.1.bn2.weight', 'layer2.1.bn2.bias', 'layer2.1.bn2.running_mean', 'layer2.1.bn2.running_var', 'layer2.1.bn2.num_batches_tracked', 'layer2.2.conv1.weight', 'layer2.2.bn1.weight', 'layer2.2.bn1.bias', 'layer2.2.bn1.running_mean', 'layer2.2.bn1.running_var', 'layer2.2.bn1.num_batches_tracked', 'layer2.2.conv2.weight', 'layer2.2.bn2.weight', 'layer2.2.bn2.bias', 'layer2.2.bn2.running_mean', 'layer2.2.bn2.running_var', 'layer2.2.bn2.num_batches_tracked', 'layer2.3.conv1.weight', 'layer2.3.bn1.weight', 'layer2.3.bn1.bias', 'layer2.3.bn1.running_mean', 'layer2.3.bn1.running_var', 'layer2.3.bn1.num_batches_tracked', 'layer2.3.conv2.weight', 'layer2.3.bn2.weight', 'layer2.3.bn2.bias', 'layer2.3.bn2.running_mean', 'layer2.3.bn2.running_var', 'layer2.3.bn2.num_batches_tracked', 'layer3.0.conv1.weight', 'layer3.0.bn1.weight', 'layer3.0.bn1.bias', 'layer3.0.bn1.running_mean', 'layer3.0.bn1.running_var', 'layer3.0.bn1.num_batches_tracked', 'layer3.0.conv2.weight', 'layer3.0.bn2.weight', 'layer3.0.bn2.bias', 'layer3.0.bn2.running_mean', 'layer3.0.bn2.running_var', 'layer3.0.bn2.num_batches_tracked', 'layer3.0.downsample.0.weight', 'layer3.0.downsample.1.weight', 'layer3.0.downsample.1.bias', 'layer3.0.downsample.1.running_mean', 'layer3.0.downsample.1.running_var', 'layer3.0.downsample.1.num_batches_tracked', 'layer3.1.conv1.weight', 'layer3.1.bn1.weight', 'layer3.1.bn1.bias', 'layer3.1.bn1.running_mean', 'layer3.1.bn1.running_var', 'layer3.1.bn1.num_batches_tracked', 'layer3.1.conv2.weight', 'layer3.1.bn2.weight', 'layer3.1.bn2.bias', 'layer3.1.bn2.running_mean', 'layer3.1.bn2.running_var', 'layer3.1.bn2.num_batches_tracked', 'layer3.2.conv1.weight', 'layer3.2.bn1.weight', 'layer3.2.bn1.bias', 'layer3.2.bn1.running_mean', 'layer3.2.bn1.running_var', 'layer3.2.bn1.num_batches_tracked', 'layer3.2.conv2.weight', 'layer3.2.bn2.weight', 'layer3.2.bn2.bias', 'layer3.2.bn2.running_mean', 'layer3.2.bn2.running_var', 'layer3.2.bn2.num_batches_tracked', 'layer3.3.conv1.weight', 'layer3.3.bn1.weight', 'layer3.3.bn1.bias', 'layer3.3.bn1.running_mean', 'layer3.3.bn1.running_var', 'layer3.3.bn1.num_batches_tracked', 'layer3.3.conv2.weight', 'layer3.3.bn2.weight', 'layer3.3.bn2.bias', 'layer3.3.bn2.running_mean', 'layer3.3.bn2.running_var', 'layer3.3.bn2.num_batches_tracked', 'layer3.4.conv1.weight', 'layer3.4.bn1.weight', 'layer3.4.bn1.bias', 'layer3.4.bn1.running_mean', 'layer3.4.bn1.running_var', 'layer3.4.bn1.num_batches_tracked', 'layer3.4.conv2.weight', 'layer3.4.bn2.weight', 'layer3.4.bn2.bias', 'layer3.4.bn2.running_mean', 'layer3.4.bn2.running_var', 'layer3.4.bn2.num_batches_tracked', 'layer3.5.conv1.weight', 'layer3.5.bn1.weight', 'layer3.5.bn1.bias', 'layer3.5.bn1.running_mean', 'layer3.5.bn1.running_var', 'layer3.5.bn1.num_batches_tracked', 'layer3.5.conv2.weight', 'layer3.5.bn2.weight', 'layer3.5.bn2.bias', 'layer3.5.bn2.running_mean', 'layer3.5.bn2.running_var', 'layer3.5.bn2.num_batches_tracked', 'layer4.0.conv1.weight', 'layer4.0.bn1.weight', 'layer4.0.bn1.bias', 'layer4.0.bn1.running_mean', 'layer4.0.bn1.running_var', 'layer4.0.bn1.num_batches_tracked', 'layer4.0.conv2.weight', 'layer4.0.bn2.weight', 'layer4.0.bn2.bias', 'layer4.0.bn2.running_mean', 'layer4.0.bn2.running_var', 'layer4.0.bn2.num_batches_tracked', 'layer4.0.downsample.0.weight', 'layer4.0.downsample.1.weight', 'layer4.0.downsample.1.bias', 'layer4.0.downsample.1.running_mean', 'layer4.0.downsample.1.running_var', 'layer4.0.downsample.1.num_batches_tracked', 'layer4.1.conv1.weight', 'layer4.1.bn1.weight', 'layer4.1.bn1.bias', 'layer4.1.bn1.running_mean', 'layer4.1.bn1.running_var', 'layer4.1.bn1.num_batches_tracked', 'layer4.1.conv2.weight', 'layer4.1.bn2.weight', 'layer4.1.bn2.bias', 'layer4.1.bn2.running_mean', 'layer4.1.bn2.running_var', 'layer4.1.bn2.num_batches_tracked', 'layer4.2.conv1.weight', 'layer4.2.bn1.weight', 'layer4.2.bn1.bias', 'layer4.2.bn1.running_mean', 'layer4.2.bn1.running_var', 'layer4.2.bn1.num_batches_tracked', 'layer4.2.conv2.weight', 'layer4.2.bn2.weight', 'layer4.2.bn2.bias', 'layer4.2.bn2.running_mean', 'layer4.2.bn2.running_var', 'layer4.2.bn2.num_batches_tracked', 'fc.weight', 'fc.bias'])\n"]},{"data":{"text/plain":["\u003cAll keys matched successfully\u003e"]},"execution_count":63,"metadata":{},"output_type":"execute_result"}],"source":["model_drive_path = \"drive/MyDrive/my_resnet_model\"\n","\n","model_dict = torch.load(model_drive_path, map_location=torch.device(devic))\n","print(model_dict.keys())\n","\n","model.load_state_dict(model_dict)"]},{"cell_type":"markdown","metadata":{"id":"2XglWGnYqRAk"},"source":["`print_eval_update()` is used to print the number of samples seen and accuracy in `evaluate()`."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"ZunQK6kIy_I6"},"outputs":[],"source":["def print_eval_update(num_correct, current_samples, total_num_samples):\n","  print(\"num_correct:\", num_correct)\n","  print(\"current_num_samples:\", current_samples)\n","  print(\"current_accuracy: {:.4f}\".format(num_correct/current_samples))\n","  print(\"total_accuracy: {:.4f}\".format(num_correct/total_num_samples))\n","  print()"]},{"cell_type":"markdown","metadata":{"id":"LC3U2-9cOpMz"},"source":["`evaluate()` evaluates the model's performance on the validation and testing datasets, it also saves a confusion matrix of the results\n","\n","Comment out the call to `print_eval_update()` if you want to reduce the output"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"LKTEbOZBnxnm"},"outputs":[],"source":["def evaluate(model, device, data_loader, subset_name, num_samples_subset, classes):\n","\n","  model.eval()\n","\n","  num_classes = len(classes)\n","  correct = 0\n","  samples = 0\n","  all_preds = []\n","  all_actual = []\n","\n","  for inputs, labels in data_loader:\n","    outputs = model(inputs)\n","    _, preds = torch.max(outputs, 1)\n","    correct += torch.sum(preds == labels.data)\n","    \n","    _preds = preds.data.cpu().numpy()\n","    _labels = labels.data.cpu().numpy()\n","    all_preds.extend(_preds)\n","    all_actual.extend(_labels)\n","\n","    samples += len(preds)\n","\n","    if samples % 100 == 0:\n","      print_eval_update(correct.item(), samples, num_samples_subset)\n","  \n","  accuracy = correct.item()/num_samples_subset\n","  print(\"\\naccuracy\", accuracy)\n","\n","  conf_matrix = confusion_matrix(all_actual, all_preds)\n","  cmp = ConfusionMatrixDisplay(conf_matrix)\n","  fig, ax = plt.subplots(figsize=(10,10))\n","  cmp.plot(ax=ax)\n","  plt.savefig(f'/content/{subset_name}_confusion_matrix.png')\n","  \n"]},{"cell_type":"markdown","metadata":{"id":"UjqEoRA5Qwm2"},"source":["Validation Dataset Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cHvU99cZQv1f"},"outputs":[],"source":["print(\"valid total samples:\", num_samples[\"valid\"], \"\\n\")\n","evaluate(model, device, valid_loader, \"valid\", num_samples[\"valid\"], classes)"]},{"cell_type":"markdown","metadata":{"id":"a_KRuyDgQ3sK"},"source":["Test Dataset Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-QfbuxQbQ3Dp"},"outputs":[],"source":["print(\"test total samples:\", num_samples[\"test\"], \"\\n\")\n","evaluate(model, device, test_loader, \"test\", num_samples[\"test\"], classes)"]},{"cell_type":"markdown","metadata":{"id":"nbPIHyRYRig1"},"source":["Test Examples"]},{"cell_type":"markdown","metadata":{"id":"cF3CeaJxSomO"},"source":["`generate_sample_subset()` creates a subset of the `indian_birds_dataset`, specifically a directory in the same structure as `indian_birds_dataset`, given a list of images"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oqzVk7o0b3ML"},"outputs":[],"source":["def generate_sample_subset(filepaths, subset_dataset_path):\n","  import glob\n","  \n","  if os.path.isdir(subset_dataset_path) == False:\n","    os.makedirs(subset_dataset_path)\n","\n","  dirs = glob.glob(dataset_path + \"/*\")\n","  new_images = []\n","  for dir in dirs:\n","    dir_basename = os.path.basename(dir)\n","    dir_path = os.path.join(subset_dataset_path, os.path.basename(dir))\n","    if os.path.isdir(dir_path) == False:\n","      os.makedirs(dir_path)\n","  \n","  for file in filepaths:\n","    og_filepath = os.path.join(dataset_path, file)\n","    new_filepath = os.path.join(subset_dataset_path, file)\n","    if os.path.isfile(og_filepath) == True and os.path.isfile(new_filepath) == False:\n","      shutil.copyfile(og_filepath, new_filepath)\n","      if os.path.isfile(new_filepath) == False:\n","        print(f\"Failed to copy over {og_filepath}.\")\n","      else:\n","        print(new_filepath)\n","      new_images.append(new_filepath)\n","  print(\"Done creating subset of dataset.\")\n","  return new_images\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"aSVV2OhovvGn"},"source":["Creates the sample dataset `indian_birds_dataset_subset`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TLsxCKh7RkZN"},"outputs":[],"source":["subset_dataset_path = \"/content/indian_birds_dataset_subset\"\n","\n","#Some test examples pulled from the Test Dataset\n","incorrect = [\"Indian Peacock/ML347405821.jpg\", \n","             \"House Crow/ML503964861.jpg\", \n","             \"Indian Grey Hornbill/ML77061781.jpg\", \n","             \"Brown-Headed Barbet/ML290838891.jpg\", \n","             \"Gray Wagtail/ML313524561.jpg\", \n","             \"Red-Wattled Lapwing/ML98206961.jpg\", \n","             \"Ruddy Shelduck/ML396122561.jpg\", \n","             \"House Crow/ML110722671.jpg\", \n","             \"Indian Roller/ML183145991.jpg\"]\n","\n","correct = [ \"Indian Grey Hornbill/ML106408871.jpg\", \n","           \"Indian Peacock/ML359756441.jpg\", \n","           \"Common Myna/ML268701161.jpg\", \n","           \"Common Rosefinch/ML72047811.jpg\", \n","           \"White-Breasted Waterhen/ML227087131.jpg\", \n","           \"White Wagtail/ML350263231.jpg\", \n","           \"House Crow/ML166883151.jpg\", \n","           \"Northern Lapwing/ML414357021.jpg\", \n","           \"Indian Pitta/ML528943911.jpg\", \n","           \"Asian Green Bee-Eater/ML196703651.jpg\",\n","           \"Common Kingfisher/ML204444751.jpg\",\n","           \"Rufous Treepie/ML85279711.jpg\",\n","           \"Sarus Crane/ML136277331.jpg\",\n","           \"White Wagtail/ML152781481.jpg\",\n","           \"Coppersmith Barbet/ML79408821.jpg\",\n","           \"Hoopoe/ML205105201.jpg\",\n","           \"Cattle Egret/ML388884741.jpg\",\n","           \"White-Breasted Kingfisher/ML205857601.jpg\",\n","           \"Forest Wagtail/ML397067501.jpg\",\n","           \"Common Tailorbird/ML166735161.jpg\",\n","           \"Jungle Babbler/ML32855221.jpg\"]\n","\n","delete_dir(os.path.join(subset_dataset_path, \"indian_birds_dataset\"))\n","\n","subset_image_paths = generate_sample_subset(incorrect + correct, subset_dataset_path)"]},{"cell_type":"markdown","metadata":{"id":"mHwl-5Ohv0Us"},"source":["Prepares the sample dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XJFomV64_37F"},"outputs":[],"source":["subset_transform = transforms.Compose([\n","    transforms.Resize(input_size),\n","    transforms.CenterCrop(input_size),\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","])\n","\n","subset_dataset = datasets.ImageFolder(subset_dataset_path, transform=subset_transform)\n","\n","subset_loader = DataLoader(subset_dataset, batch_size=1, num_workers=num_workers)"]},{"cell_type":"markdown","metadata":{"id":"LflCSj-swGwo"},"source":["`evaluate_subset()` evaluates the sample dataset using the model and prints out in input image with predicted and actual class"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_jugAhpEQlBD"},"outputs":[],"source":["def evaluate_subset(model, device, data_loader, idx_to_class):\n","\n","  model.eval()\n","\n","  i = 0\n","  for inputs, label in data_loader:\n","    outputs = model(inputs)\n","    _, pred = torch.max(outputs, 1)\n","    label = label[0].item()\n","    pred = pred[0].item()\n","\n","    img_path = data_loader.dataset.samples[i][0]\n","    img = cv2.imread(img_path)\n","    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) \n","\n","    plt.imshow(img)\n","    plt.show()\n","    print(img_path)\n","    print(\"actual:\", label, idx_to_class[label])\n","    print(\"predicted:\", pred, idx_to_class[pred])\n","    print(\"---\")\n","\n","    i += 1"]},{"cell_type":"markdown","metadata":{"id":"VCPYNUscCDBN"},"source":["Show Test Examples (Scroll through the output, there are a lot of example images)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tJ7j8e7vCCCN"},"outputs":[],"source":["evaluate_subset(model, device, subset_loader, idx_to_class)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyN105VD+JKQ+vp0Ags/ihRn","name":"","version":""},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}